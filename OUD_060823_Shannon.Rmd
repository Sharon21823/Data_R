---
title: "Shannon calculation"
author: "Sharon"
date: 'May 11th 2023'
output: 
  html_document:
    toc: true
    number_sections: true
classoption: 
- bookmarksnumbered
---

# Start
_____________________

Plots that will be used are marked with "> use"

## Packages
_____________________

Load at the beginning of every session.

```{r}
library(readxl)
library(ggplot2)
library(carData)
library(car)
library(RColorBrewer)
library(htmltools)
library(ggpubr)
library(gridExtra) #for combining plots
library(cowplot) #for combining plots
library(reshape2) #for reordering variable for ggplots
```

## Data
_____________________

Within the Stoepplantjes dataset (stoep), three location scales are established: location (street with or without number, buildings, etc), zip code and city. Each scale has its own advantages. On one hand, the more specific the scale, the more information is encoded in its data. On the other hand, it is harder to validate a specific scale. Besides the best scale for the Stoepplantjes Data, the locations will be matched with data from the Floron Eindejaars Plantenjacht (PJ). The possibility to find matching locations in the PJ set is not weighted in the selection for the best scale in the Stoep data. However, for the Shannon diversity analysis it is important that enough locations can be matched. 

The scale selection was done by data analysis, statistical rules of thumb ,and critical thinking. The summary of this selection is as followed:

* City: 
  + (+) All data has city info
  + (+) Highest possibility to find matching data in PJ data
  - (-) Not specific
  
* Zip code: 
  + (+) Relative specific
  - (-) Much data lost
  
* Location: 
  + (+) Most specific
  - (-) Hard to order(*)
  - (-) Much data lost
  
(*) The street or builiding discription can vary a lot (e.g. with or without house number, with or without additional building info, etc). If the location description does not match it will be devided into different factor levels. Resulting in labour intensive valdition (by hand) through the levels.

City and zip code scale were selected as most valuable and useful. For these scales, the location selection was done in Excel ('date_Location selection.xlsx' & 'date_StoepVsFloron.xlsx'). A more detailed explanation on the establishment of the Shannon can be found in 'date_analysis.docx'. 

The final selection of the Shannon diversity data can be found in 'date_Location selection.xlsx' in tabs: 'ShannonDiv' & 'ShannonDiv_selection'. The difference between these tabs is that the selection tab excludes all locations with < 30 observations (not statistically valid when used). All data that is used in R was copied to one file: 'date_summaryR.xlsx'. This includes all data above, and data used in other R files of the project. The data that will be loaded into R is the Shannon diversity selection data: 

```{r}
# Shannon diversity data
Shannon <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_11_summaryR.xlsx", 
    sheet = "ShannonDiv_selection")
```

For each data point the following variables are included:

* The data set that was used for the establishing of the shannon diversity,
* The scale of the data (city or zip code),
* The name of the location (either a city or a zip code),
* The total plant abundance,
* The Shannon diversity (H/relative diversity), 
* The species richness,
* If the location has more than 30 observations, and could thus be marked as statistically valid when used in a analysis (note that all non statistically valid locations are excluded from the selection data 'ShannonDiv_selection' and are only included in 'ShannonDiv'), 
* If both versions of a location (stoep and PJ) are present in the data and thus comparable (if N: the location can only be used to compare within the same data set (within) and cannot be compared to its corresponding location in the other data set (between)), as can be seen in the table below.

```{r}
print(Shannon)
```

_____________________

# Analysis Intro
_____________________

In general a model goes like: 
   $$response~<-~explanatory~+~explanatory~+~etc$$
For the analysis of the shannon diversity this results in:
   $$Shannon~<-~location~+~dataset~+~etc$$

As locations is an explanatory factor, an ANCOVA will be used for the analysis of the Shannon diversity

## Subset creation

First, the data will be selected per variable level of interest by creating subsets: 

**Subset** | **Dataset** | **Scale** |**Additional**
--------------- | ------------|----------|-------------
ShannonSubset1 | Stoep | All | -
ShannonSubset2 | PJ | All | - 
ShannonSubset3 | All | City | - 
ShannonSubset4 | All | City | Only if comparable = yes
ShannonSubset5| Stoep | City |-   
ShannonSubset6 | PJ | City | - 
ShannonSubset7 | All | Zip | - 
ShannonSubset8 | Stoep | Zip | -  

                
No subsets are made for the all (comparable) zip codes in the PJ set due to lack of data. 

*Subset codes*

```{r}
ShannonSubset1 <- Shannon[which(Shannon$Dataset=='Stoepplantjes'),] # 
ShannonSubset2 <- Shannon[which(Shannon$Dataset=='EindejaarsPlantenjacht'),] #
ShannonSubset3 <- Shannon[which(Shannon$Scale=='City'),] # 
ShannonSubset4 <- Shannon[which(Shannon$Scale=='City' & Shannon$Comparable=='Y'),] #
ShannonSubset5 <- Shannon[which(Shannon$Scale=='City' & Shannon$Dataset=='Stoepplantjes'),]
ShannonSubset6 <- Shannon[which(Shannon$Scale=='City' & Shannon$Dataset=='EindejaarsPlantenjacht'),]  #
ShannonSubset7 <- Shannon[which(Shannon$Scale=='Zip'),] #
ShannonSubset8 <- Shannon[which(Shannon$Scale=='Zip' & Shannon$Dataset=='Stoepplantjes'),] #
```

> In all subsets, species diversity is included. Both the relative diversity and the species richness can be tested, using these subsets. 
If it would add value. All of the subsets can also be used to analyse the abundance per location.

_______________

# Shannon diversity

For the analysis of any of the subsets, the models need to be analyzed to see if there is need for any editing. See Chapter 'Data Prints' for an overview on the different subsets.

__________________

**General Workflow**

@. Data Analysis 
  - Data diagnostics (quality of data)
  - Data structure and summary
  - Visual inspection (plot), if needed
  - Model specification
@. Model diagnostics
  - Homogeneity of variance 
  - Normal distribution residuals
  - Check for outliers -> extreme values
@. Model statistics
  - Run 'correct' test
  - Visualization of data
  
__________________

**Used codes**

Below, the codes that were used in this file are explained:

```{r, eval=F}
#Homogeneity variance check 1/2 
residualPlots(Model, layout = c(2,1), id = TRUE)

#Homogeneity variance check 2/2 
ncvTest(Model) 

#Normal distribution residuals 1/2
shapiro.test(Model$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
qqPlot(Model$residuals, id = TRUE) 

#Extreme values 1/2
influenceIndexPlot(Model) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
which(hatvalues(Model) > 2.5*mean(hatvalues(Model))) #check which hatvalues exceed the 2.5-times-mean limit 
hatvalues(Model)

#Extreme values 2/2
outlierTest(Model) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

__________________

**Homogeneity variance check 1/2**

* The residualPlots create 2 or 3 plots in which the distribution of residuals is plotted. This plot is used as visual test for the homogeneity of the variance
* 'id = TRUE' is used to call the names of the most extreme value in the plot

**Homogeneity variance check 2/2** 

* ncvTest tests the homogeneity of the variance
* H0: homoscedacity

**Normal distribution residuals 1/2**

* shapiro.test is used to test the normallity of the residuals
* H0: residuals are normally distributed

**Normal distribution residuals 2/2**

* In the qqPlot the residuals of the model are plotted to see if residuals are within confidence region and thus normally distributed 
* 'id = TRUE' is used to call the names of the most extreme value in the plot

**Extreme values 1/2**

* influenceIndexPlot gives 4 plots with the Cook's distance, the Standardized residuals, Bonferroni p-value, and hat-values
* These plots are used for the identification of potential outliers
* Motivation for taking out extreme value as outlier if Cook's distance > 0.5
* Motivation for taking out extreme value as outlier if hat-value > 2.5x hat-value-mean
* Motivation for taking out extreme value as outlier if Bonferroni p-value < 0.05 (see Extreme values 2/2)
* If only one of the above is happening, it should be discussed if the value is worth taking out
* 'which(hatvalues...' is used to call the data that exceed the 2.5x-hat-value-mean limit. 
* Identifying the exact hat-value with 'hatvalues'

**Extreme values 2/2**

* The outlierTest calls the p-value and Bonferroni p of extreme points (potential outliers)
* Motivation for taking out extreme value as outlier if Bonferroni p-value < 0.05 is motivation to call extreme point an outlier. Often dispite the results of the influenceIndexPlot. 

____________________

## Data Analysis

Loading the data and checking its structure are also part of the data analysis part. However, these tasks were done previously (earlier in this file or in excel during the data editing) and are thus not included in this section. Further, all NA values were taken out of the data before importing the data into R. The only step left in this section is model specification. 

### Model specification
________________________

For each subset of the Shannon data, a seperate model is created. These models are named corresponding to the name of the subset (Model_ShanSub#). All starting models will be Linear Models. Model diagnositics might suggest a different model, resulting in the creation of a new type of model (Model_ShanSub#_2, Model_ShanSub#_3, etc.). An overview of the models that gave not errors and did not need any (more) editing can be found in '3.2.7 Final Models'. These models are used for the Model statistics in the next chapter.

**Round 1**

```{r}
#Model specification per subset
Model_ShanSub1 <- lm(Rel_div ~ Scale + Location + Total_abun, data = ShannonSubset1)  #Stoep
Model_ShanSub2 <- lm(Rel_div ~ Scale + Location + Total_abun, data = ShannonSubset2)  #PJ
Model_ShanSub3 <- lm(Rel_div ~ Dataset + Location + Total_abun, data = ShannonSubset3) #City 
Model_ShanSub4 <- lm(Rel_div ~ Dataset + Location + Total_abun, data = ShannonSubset4)  #City and comparable between Stoep and PJ
Model_ShanSub5 <- lm(Rel_div ~ Location + Total_abun, data = ShannonSubset5)  #Stoep, city
Model_ShanSub6 <- lm(Rel_div ~ Location + Total_abun, data = ShannonSubset6) #PJ, city
Model_ShanSub7 <- lm(Rel_div ~ Dataset + Location + Total_abun, data = ShannonSubset7)#Zip  
Model_ShanSub8 <- lm(Rel_div ~ Location + Total_abun, data = ShannonSubset8)  #Stoep, zip
```

> Most of these models turned out to be not usable due to errors. - See '3.2.2. Problem inspection' for more info.

________________________

**Round 2 - 3**

> Used to test where the errors came from - See '3.2.2. Problem inspection'.

________________________

**Round 4**

For further explanation, on e.g. names of the models, see '3.2.2. Problem inspection'. 

```{r}
#Model specification round 4
Model_ShanSub3.4 <- lm(Rel_div ~ Dataset + Location , data = ShannonSubset3)  #Both datasets, City scale (still gave error, thus the models below were created)

#Model specification round 4.1
Model_ShanSub3.4.1 <- lm(Rel_div ~ Dataset , data = ShannonSubset3) #Both datasets, City scale 

#Model specification round 4.2
Model_ShanSub3.4.2 <- lm(Rel_div ~ Location , data = ShannonSubset3) #Both datasets, City scale 

#Model specification round 4.3
Model_ShanSub3.4.3 <- lm(Rel_div ~ Location , data = ShannonSubset4)  #Both datasets, City scale only comparable
```

See '3.2.4 Problem inspection' for the explanation of model created below. See '4.2 Different Subsets' for an overview of all created subsets.  

```{r}
#Upload new (correct) data 
ShannonSubset9 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_11_summaryR.xlsx", sheet = "ShannonDiv_CompCityOnly")

#Model specification round 4.3.2
Model_ShanSub9.4.3.2 <- lm(Rel_div ~ Location , data = ShannonSubset9)  #Both datasets, City scale only comparable - correct data!
```

**Round 5**

For further explanation see 'Round 4.3.2' >'Next Step' and 'Round 5'. 
```{r}
#Upload new (correct) data 
ShannonSubset10 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_11_summaryR.xlsx", sheet = "ShannonDiv_zip")

#Model specification round 5
Model_ShanSub10.5.1 <- lm(Rel_div ~ Location , data = ShannonSubset10)  #Only stoep data, Zip scale
```

```{r}
#Upload new (correct) data 
ShannonSubset11 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_11_summaryR.xlsx", sheet = "ShannonDiv_zip")

#Model specification round 5
Model_ShanSub11.5 <- lm(Rel_div ~ Location , data = ShannonSubset11)  #Only stoep data, Zip scale
```
________________________

## Model diagnostics

The next section is dedicated to inspecting the characteristics of the models and to investigate if the models are usable, reliable, or in need of data editing. This is done per model. 
___________________

### Round 1

**Model_ShanSub1**

```{r, eval=F}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_ShanSub1, layout = c(3,1), id = TRUE)
ncvTest(Model_ShanSub1) # p < 0.05 means heteroscedacity

#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_ShanSub1$resid) # H0: residuals are normally distributed
```
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub1$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r,eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub1) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub1) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> The residuals of Model_ShanSub1 give an error in the residualplots, shapiro-, and outliertests (therefore these are noted in a section that is not loaded). After inspection: the residuals are all '0' in thus subset. That all residuals are exactly '0' implicates that the model would be a perfect fit (also note that all the data aligns with the blue line). It is unlikely that the model fits perfect. I will inspect this later on.

___________________

**Model_ShanSub2**

```{r, eval=F}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_ShanSub2, layout = c(3,1), id = TRUE)
ncvTest(Model_ShanSub2) # p < 0.05 means heteroscedacity

#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_ShanSub2$resid) # H0: residuals are normally distributed
```
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub2$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r,eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub2) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub2) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> The same problem as for Model_ShanSub1 appears here... These two models both include all data of one dataset. 

___________________

**Model_ShanSub3**

```{r}
#Homogeneity variance check 1/2
residualPlots(Model_ShanSub3, layout = c(2,1), id = TRUE)
```
* The Total abundance plot (top) has dense data at the lower end op de scale. _Saturation? Outlier sensitive?_
* #5 and #45 id
```{r}
#Homogeneity variance check 2/2
ncvTest(Model_ShanSub3) # p < 0.05 means heteroscedacity (not desired)
```
* p >>> 0.05 -> Homoscedacity in variance
```{r}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub3$resid) # H0: residuals are normally distributed
```
* p >> 0.05 -> Residual normally distributed
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub3$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
* Not linear distribution. Does not seem to be an issue, but it is remarkable (sinus)
* #5 and #45 id
```{r, eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub3) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
* Eventhough this code gives an error, the Cook's distance did load in R
* Highest Cook's around 0.15 (#5 and #45) so no motivation to take out data based on the Cook's distance
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub3) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* #45 has p < 0.05. However, no Bonferroni p < 0.05, so no motivation to take out data

> influenceIndexPlots gives error: 'Error in xy.coords(x, y, xlabel, ylabel, log) : 
  'x' and 'y' lengths differ'. Meaning the plotted variables do not allign. This might be because some locations are mentioned twice and some are mentioned once, but I am not sure -> the next model will confirm or deny this

>> However, Model diagnostics no not suggest the need for data editing before used for model statistics

_______________________

**Model_ShanSub4**

```{r}
#Homogeneity variance check 1/2
residualPlots(Model_ShanSub4, layout = c(2,1), id = TRUE)
```
* The Total abundance plot (top) has dense data at the lower end op de scale. _Saturation? Outlier sensitive?_
* #4 and #39 id
```{r}
#Homogeneity variance check 2/2
ncvTest(Model_ShanSub4) # p < 0.05 means heteroscedacity (not desired)
```
* p >>> 0.05 -> Homoscedacity in variance
```{r}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub4$resid) # H0: residuals are normally distributed
```
* p >>> 0.05 -> Residual normally distributed
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub4$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
* Not linear distribution. Does not seem to be an issue, but it is remarkable (sinus)
* #4 and #39 id
```{r, eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub4) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
* Eventhough this code gives an error, the Cook's distance did load in R
* Highest Cook's around 0.15 (#4 and #39) so no motivation to take out data based on the Cook's distance
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub4) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* #4 has p < 0.05. However, Bonferroni p >>> 0.05, so no motivation to take out data

> influenceIndexPlot gives the same error. Same data as ShannonSubset3, but only with locations that are present in the both datasets. I do not know what the error is based on. I will further inspect this later. 

>> However, Model diagnostics no not suggest the need for data editing before used for model statistics

_________________________

**Model_ShanSub5**

```{r,eval=F}
#Homogeneity variance check 1/2 - NOT LOADED
residualPlots(Model_ShanSub5, layout = c(2,1), id = TRUE)
```
```{r,eval=F}
#Homogeneity variance check 2/2 - NOT LOADED
ncvTest(Model_ShanSub5) # p < 0.05 means heteroscedacity (not desired)
```
```{r,eval=F}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub5$resid) # H0: residuals are normally distributed
```
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub5$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
Same issue as Model_ShanSub1&2...
```{r, eval=F,}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub5) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
Error in plot.window(...) : need finite 'ylim' values
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub5) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> Same issue as earlier. All these models have in common that Dataset is not included as variable since they only contain one of the datasets.

_________________________

**Model_ShanSub6**

```{r, eval=F}
#Homogeneity variance check 1/2
residualPlots(Model_ShanSub6, layout = c(2,1), id = TRUE)
```
```{r, eval=F}
#Homogeneity variance check 2/2
ncvTest(Model_ShanSub6) # p < 0.05 means heteroscedacity (not desired)
```
```{r, eval=F}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub6$resid) # H0: residuals are normally distributed
```
Error in shapiro.test(Model_ShanSub6$resid) : 
  all 'x' values are identical
```{r, eval=F}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub6$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r, eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub6) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
Error in plot.window(...) : need finite 'ylim' values
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub6) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> Same issue... If it is indeed the issue that the dataset is all named the same (is this 'x'? - see shapiro.test), the next model will not have this problem, but Model_ShanSub8 will.

_______________________

**Model_ShanSub7**

```{r,eval=F}
#Homogeneity variance check 1/2 - NOT LOADED
residualPlots(Model_ShanSub7, layout = c(2,1), id = TRUE)
```
```{r,eval=F}
#Homogeneity variance check 2/2 - NOT LOADED
ncvTest(Model_ShanSub7) # p < 0.05 means heteroscedacity (not desired)
```
```{r, eval=F}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub7$resid) # H0: residuals are normally distributed
```
Still the same error
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub7$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
Same issue..
```{r, eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub7) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
Error in plot.window(...) : need finite 'ylim' values
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub7) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> Theory on last model disproven. Maybe I should take a different approach on the models. First, lets run the last model

_________________________

**Model_ShanSub8**

```{r eval=F}
#Homogeneity variance check 1/2
residualPlots(Model_ShanSub8, layout = c(2,1), id = TRUE)
```
```{r eval=F}
#Homogeneity variance check 2/2
ncvTest(Model_ShanSub8) # p < 0.05 means heteroscedacity (not desired)
```
```{r eval=F}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub8$resid) # H0: residuals are normally distributed
```
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub8$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
Same issue
```{r, eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub8) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
Same issue
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub8) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> Same issue

_________________________

### **Problem inspection**

The model diagnostics above (Round 1) gave quite errors. Below, you find an overview of models that did work, Including some characteristics of the data in these models. Another approach on the models is probably needed and tried out below the table.

**Model based on** | **Too much errors to used** | **Included only 1 datasets** |**Includes Zip**
--------------- | ------------|----------|-------------
ShannonSubset1 | Yes | Yes | *Yes*
ShannonSubset2 | Yes | Yes | *Yes* 
ShannonSubset3 | No | No | No
ShannonSubset4 | No | No | No
ShannonSubset5| Yes | *Yes* | No  
ShannonSubset6 | Yes | *Yes* | No
ShannonSubset7 | Yes | *Yes* | *Yes*
ShannonSubset8 | Yes | No | *Yes* 

_________________________

To test some options:

**Round 2** 

```{r}
#Model specification round 2
Model_ShanSub1.2 <- lm(Rel_div ~ Scale + Location , data = ShannonSubset1)  #Stoep
```
* Model diagnostics (deleted the code and outcome) gave same outcome... So it the problem does not come from the only continuous variable in the models 'Total_abundance' 

_________________________

**Round 3**

```{r}
#Model specification round 3
Model_ShanSub1.3 <- lm(Rel_div ~ Scale , data = ShannonSubset1)  #Stoep
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub1.3, layout = c(2,1), id = TRUE)
```
* Gives a warning message and residuals are split into two grops, which makes sense since 'Scale' is a two level factor.
```{r}
#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub1.3) # p < 0.05 means heteroscedacity (not desired)
```
* p > 0.05 -> Homoscedacity in variance 
```{r}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub1.3$resid) # H0: residuals are normally distributed
```
* p >>> 0.05 -> residuals are normally distributed
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub1.3$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
* Nicely withing confidence region
```{r}
#Extreme values 1/2
influenceIndexPlot(Model_ShanSub1.3) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
* Cook's distance is fine
* Bonferroni seems fine
* Hatvalues gives high values for #1-#10. Further inspection:
```{r}
which(hatvalues(Model_ShanSub1.3) > 2.5*mean(hatvalues(Model_ShanSub1.3))) #check which hatvalues exceed the 2.5-times-mean limit 
hatvalues(Model_ShanSub1.3)
```
* #1-#10 have higher values than limit
* All the same value: 0.10
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub1.3) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* #47 p <<< 0.05, but Bonferroni p >> 0.05 so not enough motivation to excluded data

> This model seems to work. However: this specific model is just for a subset. If models with multiple variables do not work, there is no point in using the data of the subsets instead of the total Shannon data. 

>> #1-#10 are the zip code data... I expect that these were the problem all along. Going back to the models I used in Round 1, I found out that all models with Zip data gave too much errors to use. This does not explain why Model_ShanSub5 & Model_ShanSub6 did not work. But an earlier hypothesis is about error being related to the fact that only one dataset was included. Which is the case for both Model_ShanSub5 & Model_ShanSub6.

_________________________

**Next step**

Model diagnostics will be done again, with different models (named Round 4, as Round 2 and 3 did not give the desired result). There models will be created in '3.1 Data Analysis' > '3.1.1 Model specification' > 'Round 4'. The models will be created from ShannonSubsets that did not include any zip code data, thus are the same as the original data without zip. What will be done with the zip code data will be discussed with the supervisior. 

_________________________

### Round 4

We now know that the zip code data can not be included due to errors and we expect that both dataset need to be included. This round is to check if extracting the data from the subsets without Zip codes or 1 data set could work. ShannonSubset3 includes all city data and therefore is the same as the original data excluding the zip code data.

```{r}
#Model specification round 4
Model_ShanSub3.4 <- lm(Rel_div ~ Dataset + Location , data = ShannonSubset3)  
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub3.4, layout = c(2,1), id = TRUE)
```
```{r}
#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub3.4) # p < 0.05 means heteroscedacity (not desired)
```
* p > 0.05 -> Homoscedacity in variance 
```{r}
#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub3.4$resid) # H0: residuals are normally distributed
```
* p >> 0.05 -> residuals are normally distributed
```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub3.4$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r,eval=FALSE}
#Extreme values 1/2
influenceIndexPlot(Model_ShanSub3.4) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
Still same error 
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub3.4) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> This model seems to work. However: there is still this error with the influenceIndexPlot I would like to get rid of. Therefore, the next model will be with only 1 variable at a time.

_________________________

**Round 4.1**

```{r}
#Model specification round 4.1
Model_ShanSub3.4.1 <- lm(Rel_div ~ Dataset , data = ShannonSubset3) 
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub3.4.1, layout = c(2,1), id = TRUE)
```
Warning, which is logical since Dataset is a factor. 

See what a boxplot would look like:
```{r}
Boxplot <- ggplot(ShannonSubset3, aes(Dataset, Rel_div, color=factor(Dataset), group=Dataset)) + geom_boxplot() + xlab("Dataset") + ylab("Relative diversity")
Boxplot
```
Or in contrast with the species richness
```{r}
ggplot_reldiv.sprich <- ggplot(ShannonSubset3, aes(Rel_div, Sp_richness, color=factor(Dataset), group=Dataset)) + geom_smooth(method = "lm", se=F) + xlab("Species Richness") + ylab("Relative diversity")
ggplot_reldiv.sprich
```

```{r}
#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub3.4.1) # p < 0.05 means heteroscedacity (not desired)

#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub3.4.1$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub3.4.1$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2
influenceIndexPlot(Model_ShanSub3.4.1) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```

* All looks good, except for the hatvalues. Further inspection:
```{r}
which(hatvalues(Model_ShanSub3.4.1) > 2.5*mean(hatvalues(Model_ShanSub3.4.1))) #check which hatvalues exceed the 2.5-times-mean limit 
hatvalues(Model_ShanSub3.4.1)
```
* No issue with hatvalues. Just to groups of data (non differ 2.5 times from the hatvalue mean)
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub3.4.1) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* #37 *** p value, but NS Bonferroni p

> This model does not have any problems. A t-test would be enough to test the difference between the datasets

___________________

**Round 4.2**

```{r}
#Model specification round 4.1
Model_ShanSub3.4.2 <- lm(Rel_div ~ Location , data = ShannonSubset3) 
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub3.4.2, layout = c(2,1), id = TRUE)
```
Warning, which is logical because Location is a factor with a lot of levels
```{r}
#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub3.4.2) # p < 0.05 means heteroscedacity (not desired)

#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub3.4.2$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub3.4.2$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r,eval=FALSE}
#Extreme values 1/2
influenceIndexPlot(Model_ShanSub3.4.2) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
This gives still the same error: Error in xy.coords(x, y, xlabel, ylabel, log) : 
  'x' and 'y' lengths differ
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub3.4.2) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> This model does not have any problems, except for the influenceIndexPlot. I will try what happens if only the comparable cities are used.

_________________________

**Round 4.3**

```{r}
#Model specification round 4.3
Model_ShanSub3.4.3 <- lm(Rel_div ~ Location , data = ShannonSubset4)  
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub3.4.3, layout = c(2,1), id = TRUE)
```
Warning, which is logical because Location is a factor with a lot of levels
```{r}
#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub3.4.3) # p < 0.05 means heteroscedacity (not desired)

#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub3.4.3$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub3.4.3$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r,eval=FALSE}
#Extreme values 1/2
influenceIndexPlot(Model_ShanSub3.4.3) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
This gives still the same error: Error in xy.coords(x, y, xlabel, ylabel, log) : 
  'x' and 'y' lengths differ
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub3.4.3) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```

> Same error

_________________________

### **Problem inspection**

Found an error in the dataset. The full explanation can be found in the labjournal (11 mei 2023). The cities did not line up properly. Fixed this and created a new tab: ShannonDiv_CompCityOnly to try and fix the error of Round 4.2-Round 4.3. Next, I will re-run the Round 4.3 with the new data. 

_________________________

### Round 4.3.2

The correct data and the new model was created in the Model specification. Below, a copy of these code:
```{r, eval=FALSE}
#Upload new (correct) data 
ShannonSubset9 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_11_summaryR.xlsx", sheet = "ShannonDiv_CompCityOnly")

#Model specification round 4.3.2
Model_ShanSub9.4.3.2 <- lm(Rel_div ~ Location , data = ShannonSubset9)  #Both data sets, City scale only comparable - correct data!
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub9.4.3.2, layout = c(2,1), id = TRUE)
```
Warning, which is logical because Location is a factor with a lot of levels
```{r}
#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub9.4.3.2) # p < 0.05 means heteroscedacity (not desired)

#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub9.4.3.2$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub9.4.3.2$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r}
#Extreme values 1/2
influenceIndexPlot(Model_ShanSub9.4.3.2) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```

> NO MORE ERROR!

* All looks good, except for the hatvalues. Further inspection:
```{r}
which(hatvalues(Model_ShanSub9.4.3.2) > 2.5*mean(hatvalues(Model_ShanSub9.4.3.2))) #check which hatvalues exceed the 2.5-times-mean limit 
hatvalues(Model_ShanSub9.4.3.2)
```
* All good
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub9.4.3.2) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good

> This model does not have any problems. A ANOVA would be enough to test the difference between the datasets. The error that i found will not have effected the other models, because the error was only in the variable 'Compararible' and was only incuded in ShannonSubset4 (and now in ShannonSubset9 as well)

_________________

Also check for the comparison between cities 

The correct data and the new model was created in the Model specification. Below, a copy of these code:
```{r}
#Model specification round 4.3.3
Model_ShanSub9.4.3.3 <- lm(Rel_div ~  Dataset, data = ShannonSubset9)  #Both data sets, City scale only comparable - correct data!
```

```{r,}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub9.4.3.3, layout = c(2,1), id = TRUE)
```
Warning, which is logical because Location is a factor with a lot of levels
```{r}
#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub9.4.3.3) # p < 0.05 means heteroscedacity (not desired)

#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub9.4.3.3$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub9.4.3.3$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r, eval=F}
#Extreme values 1/2
influenceIndexPlot(Model_ShanSub9.4.3.3) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
* All looks good
```{r}
which(hatvalues(Model_ShanSub9.4.3.3) > 2.5*mean(hatvalues(Model_ShanSub9.4.3.3))) #check which hatvalues exceed the 2.5-times-mean limit 
hatvalues(Model_ShanSub9.4.3.3)
```
* All good
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub9.4.3.3) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good

> This model does not have any problems

_________________________

**Next Step**

I want to see if I can make a model with only the Zip code data of the Stoepplantjes data. To be sure this will go right, a new tab was made in the 'date_summaryR.xlsx' 

_________________________

### Round 5

The new model was created in the Model specification. Below, a copy of thus code:
```{r, eval=FALSE}
#Upload new (correct) data 
ShannonSubset10 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_11_summaryR.xlsx", sheet = "ShannonDiv_zip")

#Model specification round 5
Model_ShanSub10.5.1 <- lm(Rel_div ~ Location , data = ShannonSubset10)  #Only stoep data, Zip scale
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub10.5.1, layout = c(2,1), id = TRUE)

#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub10.5.1) # p < 0.05 means heteroscedacity (not desired)

#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub10.5.1$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub10.5.1$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2
influenceIndexPlot(Model_ShanSub10.5.1) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```

* All looks good, except for the #10 on the Cook's distance (>1.2) and hat values. Further inspection:
```{r}
which(hatvalues(Model_ShanSub10.5.1) > 2.5*mean(hatvalues(Model_ShanSub10.5.1))) #check which hatvalues exceed the 2.5-times-mean limit 
hatvalues(Model_ShanSub10.5.1)
```
* #10 hat valiue is over the limit of 2.5 times. 
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub10.5.1) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good

> 10 datapoint are not much. 10 is idicated as potential outlier with the Cook's distance and the hat value limits. 

Took out #10 (in excel). Re-run the code to see if model is sufficient now.

```{r, eval=FALSE}
#Upload new (correct) data 
ShannonSubset11 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_11_summaryR.xlsx", sheet = "ShannonDiv_zip_10")

#Model specification round 5
Model_ShanSub11.5 <- lm(Rel_div ~ Location , data = ShannonSubset11)  #Only stoep data, Zip scale - #10
```

```{r}
#Homogeneity variance check 1/2 
residualPlots(Model_ShanSub11.5, layout = c(2,1), id = TRUE)

#Homogeneity variance check 2/2 
ncvTest(Model_ShanSub11.5) # p < 0.05 means heteroscedacity (not desired)

#Normal distribution residuals 1/2
shapiro.test(Model_ShanSub11.5$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub11.5$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2
influenceIndexPlot(Model_ShanSub11.5) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```

* All looks good, except for the #1 on the Cook's distance (>0.6) and hat values. Further inspection:
```{r}
which(hatvalues(Model_ShanSub11.5) > 2.5*mean(hatvalues(Model_ShanSub11.5))) #check which hatvalues exceed the 2.5-times-mean limit 
hatvalues(Model_ShanSub11.5)
```
* #1 hat valiue is **not* over the limit of 2.5 times, so there is not enough motivation (Cook's distance alone) to exclude this point from the data.
```{r}
#Extreme values 2/2
outlierTest(Model_ShanSub11.5) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good

> This model (the model without #10 (zip code 9713)) shows no issues and can be used for model statistics

_________________________

### **Final Models**

```{r}
#Model specification round 4.1
Model_ShanSub3.4.1 <- lm(Rel_div ~ Dataset , data = ShannonSubset3) #Both datasets, City scale (incl not comparable locations)

#Model specification round 4.3.2
Model_ShanSub9.4.3.2 <- lm(Rel_div ~ Location , data = ShannonSubset9)  #Both datasets, City scale only comparable - correct data!

#Model specification round 4.3.3
Model_ShanSub9.4.3.3 <- lm(Rel_div ~ Dataset, data = ShannonSubset9)  #Both data sets, City scale only comparable - correct data!

#Model specification round 5
Model_ShanSub11.5 <- lm(Rel_div ~ Location , data = ShannonSubset11)  #Only stoep data, Zip scale - #10
```
_________________________

## Model statistics

The models Model_ShanSub3.4.1, Model_ShanSub9.4.3.2, and Model_ShanSub11.5 will be used for the model statistics. These model include data from ShannonSubset3, ShannonSubset9, and ShannonSubset11. See 'Data prints' > 'Different Subsets'. In the chapters below, each model is treat separately. 

_________________________

### All city data - *Model_ShanSub3.4.1*

ShannonSubset3 includes all city data from both data sets, both the comparable cities and the cities that only have enough data in the Stoep data. A t-test should be used to analyse the difference between the data sets. Note: the difference between both sets could also be tested in with the comparable city data only (subset 9). The only difference than would be that stoep data has as much data as PJ. However, for the t-test, this extra data for Stoep (in subset3) only adds value. Therefore, it was not chosen to also test the difference in data set of the data in subset 9.

_________________________

**The model** 

```{r, eval=F}
#Model specification round 4.1
Model_ShanSub3.4.1 <- lm(Rel_div ~ Dataset , data = ShannonSubset3) #Both datasets, City scale (incl not comparable locations)
```

_________________________

**Hypotheses**

It is expected that the data of the Pavement Plant Project (here called Stoep) is sufficient to calculate the relative diversity. The data of the Eindejaars Plantenjacht by Floron (here called PJ) includes almost 10 time as much data. Despite the difference in method, e.g. limitations in time/period/etc, the data of Eindejaars Plantenjacht is used as a guideline for the reliability of the calculated relative diversity. 

H0: There is no significant difference between the relative diveristy of the Pavement Plant Project (Stoep) and Eindejaars Plantenjacht (PJ). 

_________________________

**Test**

To compare the response (relative diversity) between the two levels (Stoep & PJ) of the explanatory factor (Dataset), a t-test was used. 
```{r}
t.test(Rel_div ~ Dataset , data = ShannonSubset3)
```

> p >> 0.05 

> > **There is no significant difference between the mean of the relative diversity in the Pavement Plant Project (Stoep) and Eindejaars Plantenjacht (PJ).** 

_________________________

**Visualisation**

*FigureDiv_A*

```{r}
FigureDiv_A <- ggplot(ShannonSubset3, aes(Dataset, Rel_div, color=factor(Dataset), group=Dataset)) + 
  geom_boxplot() + labs(x="Dataset", y="Relative diversity", title = "Relative diversity per dataset", caption = "p-value = 0.159") + 
  scale_colour_discrete(name="Dataset",labels = c("Eindejaars Plantenjacht", "Stoepplantjes Project")) + 
  theme(legend.position="bottom", legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold")) + geom_signif(comparisons = list(c("EindejaarsPlantenjacht", "Stoepplantjes")), map_signif_level=TRUE)
                                      
FigureDiv_A + geom_point()

ggsave("R_figures/FigureDiv_A.pdf", width= 6, height=6) #to save the plot in working directory!
```

_________________________

### All comparable city data - *Model_ShanSub9.4.3.2 and Model_ShanSub9.4.3.3*

ShannonSubset9 includes comparable city data from both data sets. An ANOVA should be used to analyse the data.

_________________________

**The model**

```{r, eval=F}
#Model specification round 4.3.2
Model_ShanSub9.4.3.2 <- lm(Rel_div ~ Location , data = ShannonSubset9)  #Both datasets, City scale only comparable 
```

_________________________

**Hypotheses**

It is expected that there is no difference between locations (using both sets).

H0: There is no significant difference between the relative diveristy of the different locations.
    
_________________________

**Test**

To compare the response (relative diversity) between the all levels (the different locations) of the explanatory factor (Location), an anova was used.

```{r}
summary(Model_ShanSub9.4.3.2) #or summary.aov for instance
```

> p < 0.05 (*) 

> > **There is a significant difference between the relative diversity of the locations** *It seems Almere is (****) , Tilburg compared to Almere is (***)

To find out which locations differ from which, a post hoc was used. 

```{r}
TukeyHSD(aov(Model_ShanSub9.4.3.2))
```

> **Shows no significance between specific cities** 

>> I do not know why a significance was then given in the first place...

_________________________

**Visualisation** 

*FigureDiv_B*

```{r}
FigureDiv_B <- ggplot(ShannonSubset9, aes(x = reorder(Location, -Rel_div), y=Rel_div, label=Location, colour=Dataset)) + 
  geom_point() +facet_grid() + labs(x="City", y="Relative diversity", title = "Relative diversity per city", caption = "Cities are not in fixed order, \nso no regression can be visualized in the plot", subtitle ="In descending order") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=8)) + 
  theme(legend.position="bottom", legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold"))

FigureDiv_B 

ggsave("R_figures/FigureDiv_B.pdf", width= 8, height=6) #to save the plot in working directory!
```

```{r}
FigureDiv_B_2 <- ggplot(ShannonSubset9, aes(x = reorder(Location, -Rel_div), y=Rel_div, label=Location, colour=Dataset)) + 
  geom_point() +facet_grid() + labs(x="City", y="Relative diversity", title = "Relative diversity per city and dataset", subtitle ="In descending order", caption = "Cities are not in fixed order, \nso no regression can be visualized in the plot")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=8)) + 
  theme(legend.position='none', legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold")) + facet_wrap(~Dataset)

FigureDiv_B_2

ggsave("R_figures/FigureDiv_B_2.pdf", width= 10, height=6) #to save the plot in working directory!
```

_________________________

*Model_ShanSub9.4.3.3* 

ShannonSubset9 includes comparable city data from both data sets. An ANOVA should be used to analyse the data.

_________________________

**The model**


```{r, eval=F}
#Model specification round 4.3.3
Model_ShanSub9.4.3.3 <- lm(Rel_div ~ Dataset, data = ShannonSubset9)  #Both data sets, City scale only comparable - correct data!
```

_________________________

**Hypotheses**

It is expected that the data of the Pavement Plant Project (here called Stoep) does not differ from the data of the Eindejaars Plantenjacht by Floron (here called PJ). In addition it is expected that the relative diversity does not differ between locations. These expectations are tested in previous models. Resulting in the expectation that this model also does not differ between both datasets. 

H0: There is no significant difference between the relative diveristy of the locations of between the Pavement Plant Project (Stoep) and Eindejaars Plantenjacht (PJ).
    
_________________________

**Test**

To compare the response (relative diversity) between the all levels (the different locations) of the explanatory factor (Location), an anova was used.

```{r}
summary(Model_ShanSub9.4.3.3) #or summary.aov for instance
```

> !!! - XXXXXXXXXXXXXX -> ik kom er even niet uit... 
Dataset verglijken met elkaar maar met alle punten van de locaties erin... 
en locaties indiv verschillen tussen dataset?
Optie voor latere analyse: incl city opp of #kmhokken, dan kan je de grote gebruiken om de order van de cities te fixeren, een regressie lijn te maken en wat te zeggen over de shannon ivm met de grootte van de stad!!!

_________________________

**Visualisation** 

*FigureDiv_C*

```{r,eval=F}
FigureDiv_C <- ggplot(...)

FigureDiv_C 

ggsave("R_figures/FigureDiv_C.pdf", width= 6, height=6) #to save the plot in working directory!
```

_________________________


### Zip code data Stoepplantjes data - *Model_ShanSub11.5*

_________________________

**The model**

```{r, eval=F}
#Model specification round 5
Model_ShanSub11.5 <- lm(Rel_div ~ Location , data = ShannonSubset11)  #Only stoep data, Zip scale - #10
```

_________________________

**Hypotheses**

    It is expected that the data of the Pavement Plant Project (here called Stoep) is sufficient to calculate the relative diversity. The data of the Eindejaars Plantenjacht by Floron (here called PJ) includes almost 10 time as much data. Despite the difference in method, e.g. limitations in time/period/etc, the data of Eindejaars Plantenjacht is used as a guideline for the reliability of the calculated relative diversity. 

    H0: There is no significant difference between the relative diveristy of the zip code locations of the Pavement Plant Project (Stoep). 

_________________________

**Test**

To compare the response (relative diversity) between the all levels (the different locations) of the explanatory factor (Location), an anova was used.

```{r}
summary(Model_ShanSub11.5) #or summary.aov for instance
```

> No significant difference can be found

_________________________

**Visualisation**

*FigureDiv_D*

```{r}
FigureDiv_D <- ggplot(ShannonSubset11, aes(x = reorder(Location, -Rel_div), y=Rel_div, label=Location)) + 
  geom_point() +facet_grid() + labs(x="Zip code", y="Relative diversity", title = "Relative diversity per zip code", caption = "Zip codes are not in fixed order, \nso no regression can be visualized in the plot ")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=8)) + 
  theme(plot.title = element_text(size=12, face="bold"))

FigureDiv_D 

ggsave("R_figures/FigureDiv_D.pdf", width= 6, height=6) #to save the plot in working directory!
```

_________________________

# Species richness

For this section of the analysis, the same workflow as for the shannon diveristy is used (incl general workflow and codes). Additional codes that were used in this section:

**Transformation**, *Dealing with; non-equal variance/heteroscedacity*

    y = x × ϵ  -->  y = x + ϵ

  By taking log --> log ( y ) = log ( x × ϵ ) = log ( x ) + log ( ϵ )
  AKA dependent data will become independent data

- Most common:
    1. Log-transformation
    2. Arc sin Root
    3. Box Cox (power transformation)
        -> suggests which transformation might be best
        
```{r, eval=FALSE}
library(car)
boxCox(Response~Explanatory)
```

When λ =

- -2 then $Response^{-2}$ transform
- -1 then $Response^{-1}$ transform
-  0 then $log(Response)$ transform
-  1/3 then $Response^{1/3}$ transform
-  1/2 then $Response^{1/2}$ transform
-  1 then $Response^{1}$ transform
-  2 then $Response^{2}$ transform

NOTE: BoxCox will not always give a discrete number. Choose the best option. E.g. the closed line to the middle of the BoxCox and test the transformation for heteroscedacity (trail and error).

__________________

## Data Analysis

Loading the data and checking its structure are also part of the data analysis part. However, these tasks were done previously (earlier in this file or in excel during the data editing) and are thus not included in this section. Further, all NA values were taken out of the data before importing the data into R. The only step left in this section is model specification. These models can be specified from the ShannonSubset series, since these subsets also include the species richness (and abundance).

```{r}
#Upload new (fool proof) data of ShannonSubset5 
ShannonSubset12 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_15_summaryR.xlsx", sheet = "ShannonDiv_city")
```

Note: dit subset zal met terugwerkende kracht worden gebruikt in de analyse van de shannon diversity. Verder gebruik is uitlegd onder het kopje Data Print

________________________


### Model specification

________________________

For each subset of the species richness data, seperate models are created (like for the Shannon data). These models are named corresponding to the name of the subset (Model_ShanSub#). All starting models will be Linear Models. Model diagnositics might suggest a different model, resulting in the creation of a new type of model (Model_ShanSub#_2, Model_ShanSub#_3, etc.). In the section with Shannon data it was pointed out that some subsets were not useful (taken the results of the analysis of the shannon diversity into account). These subset will be used in this section.

_________________________

**Round 1**

```{r}
#Model specification per subset
Model_SpRichSub3 <- lm(Sp_richness ~ Dataset + Location, data = ShannonSubset3) #City, all
Model_SpRichSub9 <- lm(Sp_richness ~ Dataset + Location , data = ShannonSubset9)  #comparable city, both data sets 
Model_SpRichSub10 <- lm(Sp_richness ~ Location, data = ShannonSubset10)  #Stoep, zip 
Model_SpRichSub12 <- lm(Sp_richness ~ Location, data = ShannonSubset12)  #Stoep, city
```

> Only (potentially) valuable data is used.

________________________


## Model diagnostics

The next section is dedicated to inspecting the characteristics of the models and to investigate if the models are usable, reliable, or in need of data editing. This is done per model. 

___________________

### Round 1

**Model_SpRichSub3**

```{r}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub3, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub3) # p < 0.05 means heteroscedacity
```
* p <<< 0.05 
* Transformation needed

```{r}
#Transformation suggestion
boxCox(Model_SpRichSub3)
```
* Response^{-1} seems the best option

```{r}
#1st transformation model
Model_SpRichSub3.2 <- lm((Sp_richness^{-1}) ~ Dataset + Location, data = ShannonSubset3) #City, all
```

```{r}
#Re-try: Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub3.2, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub3.2) # p < 0.05 means heteroscedacity
```
* Still heteroscedastic. Try again. 

```{r}
#Transformation suggestion (again)
boxCox(Model_SpRichSub3)
```
* Try Response^{1/3}

```{r}
#2nd transformation model
Model_SpRichSub3.3 <- lm(log(Sp_richness) ~ Dataset + Location, data = ShannonSubset3) #City, all
```

```{r}
#Re-try: Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub3.3, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub3.3) # p < 0.05 means heteroscedacity
```
* No longer heteroscedacity

```{r}
#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_SpRichSub3.3$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_SpRichSub3.3$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)
```
```{r,eval=F}
#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_SpRichSub3.3) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
* Error. Which makes sense (incl cities that are included once in the set). 

```{r}
#Extreme values 2/2
outlierTest(Model_SpRichSub3.3) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good 

> No problems with the model (that we did not know before)

_________________________

**Model_SpRichSub9**

```{r}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub9, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub9) # p < 0.05 means heteroscedacity
```
* p <<< 0.05 
* Transformation needed

```{r}
#Transformation suggestion
boxCox(Model_SpRichSub9)
```
* Response^{-1} seems an option

```{r}
#1st transformation model
Model_SpRichSub9.2 <- lm((Sp_richness^{-1}) ~ Dataset + Location, data = ShannonSubset9) #City, all
```

```{r}
#Re-try: Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub9.2, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub9.2) # p < 0.05 means heteroscedacity
```
* Still heteroscedastic. Try again. 

```{r}
#Transformation suggestion (again)
boxCox(Model_SpRichSub9)
```
* Try log(Response)

```{r}
#2nd transformation model
Model_SpRichSub9.3 <- lm(log(Sp_richness) ~ Dataset + Location, data = ShannonSubset9) #City, all
```

```{r}
#Re-try: Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub9.3, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub9.3) # p < 0.05 means heteroscedacity
```
* No longer heteroscedacity

```{r}
#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_SpRichSub9.3$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_SpRichSub9.3$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_SpRichSub9.3) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
* Looks good 

```{r}
#Extreme values 2/2
outlierTest(Model_SpRichSub9.3) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good 

> No problem with the model

_________________________

**Model_SpRichSub10**

```{r}
#Homogeneity var visual and test 
residualPlots(Model_SpRichSub10, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub10) # p < 0.05 means heteroscedacity
```

```{r}
#Normal distribution residuals 1/2 
shapiro.test(Model_SpRichSub10$resid) # H0: residuals are normally distributed
```
* Residuals not normally distributed. Suggests a GLM. However. There are just 10 data points. You could argue if trying a GLM is even worth it...

```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_SpRichSub10$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2 
influenceIndexPlot(Model_SpRichSub10) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```
* Hat value of # denfinetly to high

```{r}
#Extreme values 2/2
outlierTest(Model_SpRichSub10) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* #10 not mentioned, so do not take the point out

> A GLM is needed to use this model. However, is a GLM worth the effort for 10 data points?

_________________________

**Model_SpRichSub12** 

```{r,eval=F}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub12, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub12) # p < 0.05 means heteroscedacity
```
* Shows same problem as before. I do not understand why to be fair...

```{r,eval=F}
#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_SpRichSub12$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_SpRichSub12$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_SpRichSub12) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean

#Extreme values 2/2
outlierTest(Model_SpRichSub12) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All codes give Errors. I can not figure out why...

> I can not figure out why it does not work... Might come back to this, or use the subset9

_________________________


### **Final Models**

```{r,eval=F}
#Model specification per subset
Model_SpRichSub3.3 <- lm(log(Sp_richness) ~ Dataset + Location, data = ShannonSubset3) #City, all
Model_SpRichSub9.3 <- lm(log(Sp_richness) ~ Dataset + Location, data = ShannonSubset9) #comparable city, both data sets 

Model_SpRichSub10 # only useful with GLM
Model_SpRichSub12 # might be revisted later
```

_________________________

## Model statistics

The models Model_SpRichSub3.3 and Model_SpRichSub9.3will be used for the model statistics. These model include data from ShannonSubset3 and ShannonSubset9. See 'Data prints' > 'Different Subsets'. In the chapters below, each model is treat separately. 

_________________________

### All city data - *Model_SpRichSub3.3*

ShannonSubset3 includes all city data from both data sets, both the comparable cities and the cities that only have enough data in the Stoep data. A t-test should be used to analyse the difference between the data sets. Note: the difference between both sets could also be tested in with the comparable city data only (subset 9). The only difference than would be that stoep data has as much data as PJ. However, for the t-test, this extra data for Stoep (in subset3) only adds value. Therefore, it was not chosen to also test the difference in data set of the data in subset 9.

_________________________

**The model** 

```{r, eval=F}
#Model specification round 4.1
Model_SpRichSub3.3 <- lm(log(Sp_richness) ~ Dataset + Location, data = ShannonSubset3) #Both datasets, City scale (incl not comparable locations)
```

_________________________

**Hypotheses**

    It is expected that the data of the Pavement Plant Project (here called Stoep) is sufficient to calculate the relative diversity. The data of the Eindejaars Plantenjacht by Floron (here called PJ) includes almost 10 time as much data. Despite the difference in method, e.g. limitations in time/period/etc, the data of Eindejaars Plantenjacht is used as a guideline for the reliability of the calculated relative diversity. 

    H0: There is no significant difference between the relative diveristy of the Pavement Plant Project (Stoep) and Eindejaars Plantenjacht (PJ). 

_________________________

**Test**

To compare the response (species richness) between the two levels (Stoep & PJ) of the explanatory factor (Dataset), a t-test was used. 
```{r}
t.test(Sp_richness ~ Dataset , data = ShannonSubset3)
```

> p < 0.01 

> > **There is a significant difference between the mean of the species richness in the Pavement Plant Project (Stoep) and Eindejaars Plantenjacht (PJ).** 

_________________________

**Visualisation**

*FigureDiv_E*

```{r}
FigureDiv_E <- ggplot(ShannonSubset3, aes(Dataset, Sp_richness, color=factor(Dataset), group=Dataset)) + geom_boxplot() + labs(x="Dataset", y="Species richness", title = "Species richness per dataset", caption = "p-value = 0.0109") + 
  scale_colour_discrete(name="Dataset",labels = c("Eindejaars Plantenjacht", "Stoepplantjes Project")) + theme(legend.position="bottom", legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold")) + geom_signif(comparisons = list(c("EindejaarsPlantenjacht", "Stoepplantjes")), map_signif_level=TRUE)
                                      
FigureDiv_E + geom_point()

ggsave("R_figures/FigureDiv_E.pdf", width= 6, height=6) #to save the plot in working directory!
```

________________________

### All comparable city data - *Model_SpRichSub9.3*

ShannonSubset9 includes comparable city data from both data sets. An ANOVA should be used to analyse the data.

_________________________

**The model**

```{r, eval=F}
#Model specification 
Model_SpRichSub9.3 <- lm(log(Sp_richness) ~ Dataset + Location, data = ShannonSubset9) #Both datasets, City scale only comparable 
```

_________________________

**Hypotheses**

    It is expected that there is no difference between locations (using both sets).

    H0: There is no significant difference between the relative diveristy of the different locations.
    
_________________________

**Test**

To compare the response (species richness) between the all levels (the different locations) of the explanatory factor (Location), an anova was used.

```{r}
summary(Model_SpRichSub9.3) #or summary.aov for instance
```

> p < 0.05 (*) 

> > **There is a significant difference between the relative diversity of the locations** XXXX need to follow up?

_________________________

**Visualisation** 

*FigureDiv_F*

```{r}
FigureDiv_F <- ggplot(ShannonSubset9, aes(x = reorder(Location, -Sp_richness), y=Sp_richness, label=Location, colour=Dataset)) + 
  geom_point() +facet_grid() + labs(x="City", y="Species richness", title = "Species richness per city", caption = "Cities are not in fixed order, \nso no regression can be visualized in the plot", subtitle ="In descending order") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=8)) + 
  theme(legend.position="bottom", legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold"))

FigureDiv_F 

ggsave("R_figures/FigureDiv_F.pdf", width= 8, height=6) #to save the plot in working directory!
```

```{r}
FigureDiv_F_2 <- ggplot(ShannonSubset9, aes(x = reorder(Location, -Sp_richness), y=Sp_richness, label=Location, colour=Dataset)) + 
  geom_point() +facet_grid() + labs(x="City", y="Species richness", title = "Species richness per city and dataset", subtitle ="In descending order", caption = "Cities are not in fixed order, \nso no regression can be visualized in the plot")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=8)) + 
  theme(legend.position='none', legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold")) + facet_wrap(~Dataset)

FigureDiv_F_2

ggsave("R_figures/FigureDiv_F_2.pdf", width= 10, height=6) #to save the plot in working directory!
```

_________________________

# Incl city area

It will be interesting to test the data sets and locations according to their city area. I added this to the data, creating 2 new subsets: ShannonSubset13 and ShannonSubset14. The city area was added in Excel. *Note: for this section of the analysis it emphasized that it is assumed that the locations for the data accurate.* These subsets are further explained and in the Section Data Prints. 

```{r}
#Upload new data - city data comparable
ShannonSubset13 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_15_summaryR.xlsx", sheet = "ShannonDiv_area_all")

#Upload new data - city data stoep
ShannonSubset14 <- read_excel("~/Documents/Universiteit/4. Internships/Hortus 2023/Data/Data versioning/05_15_summaryR.xlsx", sheet = "ShannonDiv_area")
```

```{r}
#New models - city data comparable
Model_SpRichSub13 <-  lm(Sp_richness ~ Dataset + Location + Area , data = ShannonSubset13)
Model_SpRichSub13.3 <- lm(Sp_richness ~ Area , data = ShannonSubset13) 
Model_ShanSub13 <-  lm(Rel_div ~ Dataset + Location + Area, data = ShannonSubset13)
Model_ShanSub13.2 <- lm(Rel_div ~ Area , data = ShannonSubset13) 

#New models - city data stoep
Model_SpRichSub14 <- lm(Sp_richness ~ Location + Area, data = ShannonSubset14)
Model_ShanSub14 <-  lm(Rel_div ~ Location + Area, data = ShannonSubset14)
```

## Model diagnostics

The next section is dedicated to inspecting the characteristics of the models and to investigate if the models are usable, reliable, or in need of data editing. This is done per model. As area data was added and the models are checked before, this will be done briefly. 

___________________

### Subset 13 

**Model_SpRichSub13**

```{r}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub13, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub13) # p < 0.05 means heteroscedacity
```
* p <<< 0.05 
* Transformation needed

```{r}
#Transformation suggestion
boxCox(Model_SpRichSub13)
```
* log(Response) seems the best option

```{r}
#1st transformation model
Model_SpRichSub13.2 <- lm(log(Sp_richness) ~ Dataset + Location + Area , data = ShannonSubset13) #City, all
```

```{r}
#Re-try: Homogeneity var visual and test 
residualPlots(Model_SpRichSub13.2, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub13.2) # p < 0.05 means heteroscedacity
```
* No longer heteroscedacity

```{r}
#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_SpRichSub13.2$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_SpRichSub13.2$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_SpRichSub13.2) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean
```

```{r}
#Extreme values 2/2
outlierTest(Model_SpRichSub13.2) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good 

___________________

**Model_ShanSub13**

```{r}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_ShanSub13, layout = c(3,1), id = TRUE)
ncvTest(Model_ShanSub13) # p < 0.05 means heteroscedacity

#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_ShanSub13$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub13$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub13) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean

#Extreme values 2/2
outlierTest(Model_ShanSub13) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good 

___________________

### Relation sp rich and area

*Model_SpRichSub13.3*

```{r}
#Homogeneity var visual and test -
residualPlots(Model_SpRichSub13.3, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub13.3) # p < 0.05 means heteroscedacity
```
* Heteroscadestic --> probably also log

```{r}
#1st transformation
Model_SpRichSub13.4 <- lm(log(Sp_richness) ~ Area , data = ShannonSubset13) 
```

```{r}
#Homogeneity var visual and test -
residualPlots(Model_SpRichSub13.4, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub13.4) # p < 0.05 means heteroscedacity
```

```{r}
#Normal distribution residuals 1/2
shapiro.test(Model_SpRichSub13.4$resid) # H0: residuals are normally distributed
```
> Suggests GLM 

```{r}
#Normal distribution residuals 2/2
library(car)
qqPlot(Model_SpRichSub13.4$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2
influenceIndexPlot(Model_SpRichSub13.4) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean

#Extreme values 2/2
outlierTest(Model_SpRichSub13.4) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good 

> GLM needed :)

**GLM**

*Model_SpRichSub13.3.2*

See for workflow, GLM at the end of the file.

1. Model specification

```{r, eval=FALSE}
Model_SpRichSub13.3.GLM <- glm(Sp_richness ~ Area, family = poisson(link = "log"), data = ShannonSubset13)
```

2. Model visualisation

```{r, eval=FALSE}
# for Count ~ Concentration + Treatment, data = Poisson
library(ggplot2)
Model_SpRichSub13.3.GLM.gg <- ggplot(ShannonSubset13, aes(Area, Sp_richness)) + 
  geom_smooth(method="glm",  method.args = list(family = poisson(link = "log"))) +
  geom_point() + ggtitle("Log linkfunction Sp richness and Area")
Model_SpRichSub13.3.GLM.gg
```

*There probably seems to be no linear regression and no continuous slope. Due to the fluctuation of slopes GLM uses rate of change. See below.*

3. Model statistics

```{r, eval = FALSE}
summary(Model_SpRichSub13.3.GLM)
```

a. *not as aspected:*
    Deviance Residuals = Variance Residuals
  - Some sort of Sum of Squares. Minimized due to the use of Maximum Likelihood.
  - Median should be around 0 and Min and Max should deviate approximately the same distance from median. 
  
b. ...
    Estimate = slope, Estimate of interaction = slope difference 
  - *Note: GLM uses rate of change instead of slope, therefore the rate of change needs to be calculated. See below. *

c. ...
    LM uses t-value, GLM uses z-value. Same calulation.

d. *Area is ****
    Concentration is N.S., but the interactions with Concentration are. Leave Conc in model.

e.  *not a perfect fit*
    Dispersion parameter Poisson is 1 means λ = 1. 
  - Expecting to have a perfect fit when Res.dev.:Df = 1:1!
  - Want to alter this value? "summary(ModelGLM, dispersion = desired value)"

f.  *Residuals deviance > Df = overdispersion = not complex enough > type 1 error*
    To be interpreted as dispersion parameter φ.
  - Deviance explained: (Null deviance - residual deviance)/Null deviance
  - Perfect fit of model when Residuals deviance = Df (see e.)
  - Residuals deviance < Df = underdispersion = too complex > Type 2 error
  - Residuals deviance > Df = overdispersion = not complex enough > type 1 error

g. *A good fit*
    Fisher Scoring: algorithm needed x steps to find the best fit.
  - A Fisher score iteration of 4-8 is good
  

```{r}
exp(0.0025639) # Exponent of slope to: estimate increase of species per unit area
```

NOTE: 1.30 means a rate +30%. So per unit area there is a 30% increase of Response (in this data set Species)

> Per unit area there is an increase of 0,002% of species richness. 

This, added to the ggplot from earlier, indicated not clear relation

___________________

*Model_ShanSub13.2*

```{r}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_ShanSub13.2, layout = c(3,1), id = TRUE)
ncvTest(Model_ShanSub13.2) # p < 0.05 means heteroscedacity

#Normal distribution residuals 1/2 - NOT LOADED
shapiro.test(Model_ShanSub13.2$resid) # H0: residuals are normally distributed

#Normal distribution residuals 2/2
library(car)
qqPlot(Model_ShanSub13.2$residuals, id = TRUE) #see if residuals are within confindence region (id=TRUE -> name point)

#Extreme values 1/2 - NOT LOADED
influenceIndexPlot(Model_ShanSub13.2) #Motivation for taking out extreme value as outlier: Labjournal error, Cook's distance > 0.5, hatvalues > 2.5 times hatvalue mean

#Extreme values 2/2
outlierTest(Model_ShanSub13.2) # Bonferroni p < 0.05 is motivation to call extreme point an outlier
```
* All good 

> No problems with the model

___________________


### Subset 14

This data set has the same errors as all other subset that only incl Stoep. Maybe it does not have enough point per level (thus location). For now, dont use this set. Might revisit later.

**Model_SpRichSub14**

```{r, eval=F}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_SpRichSub14, layout = c(3,1), id = TRUE)
ncvTest(Model_SpRichSub14) # p < 0.05 means heteroscedacity
```

___________________

**Model_ShanSub14**

```{r}
#Homogeneity var visual and test - NOT LOADED
residualPlots(Model_ShanSub14, layout = c(3,1), id = TRUE)
ncvTest(Model_ShanSub14) # p < 0.05 means heteroscedacity
```

___________________

### Final Models

```{r, eval=FALSE}
Model_SpRichSub13.2 <- lm(log(Sp_richness) ~ Dataset + Location + Area , data = ShannonSubset13) 
Model_SpRichSub13.4 <- lm(log(Sp_richness) ~ Area , data = ShannonSubset13) #needs GLM
Model_ShanSub13 <-  lm(Rel_div ~ Dataset + Location + Area, data = ShannonSubset13)
Model_ShanSub13.2 <-  lm(Rel_div ~Area, data = ShannonSubset13)
```

## Model statistics

The models Model_SpRichSub13.2 and Model_ShanSub13 will be used for the model statistics. These model include data from ShannonSubset3 and ShannonSubset9. See 'Data prints' > 'Different Subsets'. In the chapters below, each model is treat separately. 

_________________________

### All city data - *Model_SpRichSub13.2*

    ShannonSubset3 includes all city data from both data sets, both the comparable cities and the cities that only have enough data in the Stoep data. A t-test should be used to analyse the difference between the data sets. Note: the difference between both sets could also be tested in with the comparable city data only (subset 9). The only difference than would be that stoep data has as much data as PJ. However, for the t-test, this extra data for Stoep (in subset3) only adds value. Therefore, it was not chosen to also test the difference in data set of the data in subset 9.

_________________________

**Hypotheses**

    It is expected that the data of the Pavement Plant Project (here called Stoep) is sufficient to calculate the relative diversity. The data of the Eindejaars Plantenjacht by Floron (here called PJ) includes almost 10 time as much data. Despite the difference in method, e.g. limitations in time/period/etc, the data of Eindejaars Plantenjacht is used as a guideline for the reliability of the calculated relative diversity. 

    H0: There is no significant difference between the relative diveristy of the Pavement Plant Project (Stoep) and Eindejaars Plantenjacht (PJ). 

_________________________

**Test**

    To compare the response (species richness) between the two levels (Stoep & PJ) of the explanatory factor (Dataset), a t-test was used. 
    
An ANCOVA was used 

```{r}
summary(Model_SpRichSub13.2)
```

> p << 0.05

> > **There is ...** 

XXXXXXXX do post hoc?

_________________________

**Visualisation**

*FigureDiv_F*

```{r}
FigureDiv_F <- ggplot(ShannonSubset13, aes(Area, Sp_richness, color=Dataset)) + 
  geom_point() + 
  labs(x="City Area", y="Species richness", title = "Species richness in relation to city area", caption = "A GLM is needed for statistics on the relation of sp rich and area") +
  scale_colour_discrete(name="Dataset",labels = c("Eindejaars Plantenjacht", "Stoepplantjes Project")) + 
  theme(legend.position="bottom", legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold"))
                                      
FigureDiv_F 

ggsave("R_figures/FigureDiv_F.pdf", width= 6, height=6) #to save the plot in working directory!
```

_____________________

### All city data - *Model_ShanSub13*

    ShannonSubset3 includes all city data from both data sets, both the comparable cities and the cities that only have enough data in the Stoep data. A t-test should be used to analyse the difference between the data sets. Note: the difference between both sets could also be tested in with the comparable city data only (subset 9). The only difference than would be that stoep data has as much data as PJ. However, for the t-test, this extra data for Stoep (in subset3) only adds value. Therefore, it was not chosen to also test the difference in data set of the data in subset 9.

_________________________

**Hypotheses**

    It is expected that the data of the Pavement Plant Project (here called Stoep) is sufficient to calculate the relative diversity. The data of the Eindejaars Plantenjacht by Floron (here called PJ) includes almost 10 time as much data. Despite the difference in method, e.g. limitations in time/period/etc, the data of Eindejaars Plantenjacht is used as a guideline for the reliability of the calculated relative diversity. 

    H0: There is no significant difference between the relative diveristy of the Pavement Plant Project (Stoep) and Eindejaars Plantenjacht (PJ). 

_________________________

**Test**

    To compare the response (species richness) between the two levels (Stoep & PJ) of the explanatory factor (Dataset), a t-test was used. 
```{r}
t.test(Sp_richness ~ Dataset , data = ShannonSubset3)
```

> p ...

> > **There is ...** 

______________________

**Visualisation**

*FigureDiv_G*

```{r}
FigureDiv_G <- ggplot(ShannonSubset3, aes(Dataset, Sp_richness, color=factor(Dataset), group=Dataset)) + geom_boxplot() + labs(x="Dataset", y="Species richness", title = "Species richness per dataset", caption = "p-value = 0.0109") + 
  scale_colour_discrete(name="Dataset",labels = c("Eindejaars Plantenjacht", "Stoepplantjes Project")) + theme(legend.position="bottom", legend.title = element_text(size=10, face="bold"), plot.title = element_text(size=12, face="bold")) + geom_signif(comparisons = list(c("EindejaarsPlantenjacht", "Stoepplantjes")), map_signif_level=TRUE)
                                      
FigureDiv_G + geom_point()

ggsave("R_figures/FigureDiv_G.pdf", width= 6, height=6) #to save the plot in working directory!
```

___________________

# Data prints

## Shannon data set

```{r}
print(Shannon)
```

## Different Subsets

**Subset** | **Dataset** | **Scale** |**Additional**
--------------- | ------------|----------|-------------
ShannonSubset1 | Stoep | All | -
ShannonSubset2 | PJ | All | - 
ShannonSubset3 | All | City | - 
ShannonSubset4 | All | City | Only if comparable = yes - error in data
ShannonSubset5| Stoep | City |-   
ShannonSubset6 | PJ | City | - 
ShannonSubset7 | All | Zip | - 
ShannonSubset8 | Stoep | Zip | -  
ShannonSubset9 | All | City | Only if comparable = yes - correct data (new version of ShannonSubset4)
ShannonSubset10 | Stoep | Zip | Same data as ShannonSubset8, but fool proof layout
ShannonSubset11 | Stoep | Zip | Without #10
ShannonSubset12 | Stoep | City | Same data as ShannonSubset5, but fool proof layout
ShannonSubset13 | All | City | Incl city area
ShannonSubset14 | All | City | Only if comparable = yes + incl city area

```{r}
print(ShannonSubset1)
print(ShannonSubset2) 
print(ShannonSubset3) 
print(ShannonSubset4) 
print(ShannonSubset5) 
print(ShannonSubset6) 
print(ShannonSubset7) 
print(ShannonSubset8) 
print(ShannonSubset9) 
print(ShannonSubset10) 
print(ShannonSubset11) 
```




# GLM 

**Dealing with; residuals not normally distributed**

### Background

**Generalized Linear Model**, for Poisson.

*Poisson distribution* 

- For counts and waiting times.
- Same family as Norm(); exponential family. (like: binomial, negative binomial and Gamma)
- Only 1 parameter: Pois( λ ) (Instead of Norm( μ , σ )) --> if λ is low, the distribution does not even look like a normal distribution, because λ = μ and λ = σ^2.
- Likely to get Under/overdispersion; λ = μ and λ = σ^2 -- so, poissonis not able to show excess variation (due to 1 param instead of 2). 
- Numeric variable are Integer variables (counts; also a numeric variable)

*GLM Model*

Transforms the model, not response (unlike transformation). 

Build on 3 elements:

1.	The linear equation
2.	A distribution of the exponential family. - AKA Error structure
3.	A link function that links the linear equation to a distribution.

        1. Linear equatation
        η = β_0 + β_1 x_1 + β_2 x_2 + β_3 x_3 + ⋯
        expectation = linear model + NO residuals
    
        2. Distribution of exponential family
        General equation:
            f(yi; Θi, φ) = exp((yi Θi - b (Θi)) / a ϕ + c (yi ; ϕ))
            Θ = to be estimated
            ϕ = dispersion parameter (scale) 
            a, b & c dependent of distribution.
            
        - Normal dist: Θ = μ, ϕ = σ^2 and b (Θ) = σ^2 /2
        - Poisson dist: Θ = log(μ), ϕ = 1 and b(Θ) = exp(Θ)
        - Binomial dist: Θ = log(μ /(n - μ), ϕ = 1 and b(Θ) = nlog( 1 + e^Θ)

        3. Linkfunction
        Poisson:
        Identity: for big expectation values (poisson); 
        log: for strong assymetry with long tail; 
        √: for light assymerty and medium expectation values;
        
        Binominal:
        logit; and probit.
        
        See below for more. 
    
NOTE: Maximum Likelihood is used instead of Least Squares. ML fits the model by maximizing the probability of selecting the sample given a parameter and minimizes Deviance Residuals.
    
***

### Workflow

1. Data Analysis > as shown in intro
2. Model diagnostics > mostly as shown in intro

- Create model
- Homogeneity of variance 
- **Poisson (e.g.) distribution residuals** (instead of normal, as 'usual')
- Check for outliers
- Create new model: GLM
  - Start over with Model diagnostics

NOTE: The residual distribution MUST be one of the exponential family to perform a GLM.

```{r, eval=FALSE}
ModelGLM_Link <- glm(Response ~ Explanatory, family = ErrorStructure(Linkfunction), data = Data)
```

- Visualization example

```{r, eval=FALSE}
# for Count ~ Concentration + Treatment, data = Poisson
library(ggplot2)
ggplot(Poisson, aes(Concentration, Count, colour = Treatment)) + 
  geom_smooth(method="glm",  method.args = list(family = poisson(link = "log"))) +
  geom_point() + ggtitle("Log linkfunction")
```

*There probably seems to be no linear regression and no continuous slope. Due to the fluctuation of slopes GLM uses rate of change. See below.*

3. Model statistics

```{r, eval = FALSE}
summary(ModelGLM_Link)
```

*OUTPUT*

      Deviance Residuals:                                                         *a.
           Min        1Q    Median        3Q       Max  
      -2.70384  -0.67059  -0.07818   0.60572   1.84736  

      Coefficients:
                                   Estimate   Std. Error  z value   Pr(>|z|)      *b&c.
      (Intercept)                  0.57919    0.25828     2.242     0.024929 *  
      TreatmentASFT                0.34223    0.31953     1.071     0.284149    
      TreatmentFGST                0.56290    0.30348     1.855     0.063625 .  
      Concentration                0.10243    0.05592     1.832     0.067001 .    *d.
      TreatmentASFT:Concentration  0.16126    0.06655     2.423     0.015385 *  
      TreatmentFGST:Concentration  0.21453    0.06328     3.390     0.000698 ***

    (Dispersion parameter for poisson family taken to be 1)                       *e.

    Null deviance: 423.514  on 71  degrees of freedom
    Residual deviance:  66.505  on 66  degrees of freedom                         *f. 
    AIC: 324.88

    Number of Fisher Scoring iterations: 5                                        *g.

a. Deviance Residuals = Variance Residuals
  - Some sort of Sum of Squares. Minimized due to the use of Maximum Likelihood.
  - Median should be around 0 and Min and Max should deviate approximately the same distance from median. 
b. Estimate = slope, Estimate of interaction = slope difference 
  - *Note: GLM uses rate of change instead of slope, therefore the rate of change needs to be calculated. See below. *
c. LM uses t-value, GLM uses z-value. Same calulation.
d. Concentration is N.S., but the interactions with Concentration are. Leave Conc in model.
e. Dispersion parameter Poisson is 1 means λ = 1. 
  - Expecting to have a perfect fit when Res.dev.:Df = 1:1
  - Want to alter this value? "summary(ModelGLM, dispersion = desired value)"
f. To be interpreted as dispersion parameter φ.
  - Deviance explained: (Null deviance - residual deviance)/Null deviance
  - Perfect fit of model when Residuals deviance = Df (see e.)
  - Residuals deviance < Df = underdispersion = too complex > Type 2 error
  - Residuals deviance > Df = overdispersion = not complex enough > type 1 error
g. Fisher Scoring: algorithm needed x steps to find the best fit.
  - A Fisher score iteration of 4-8 is good
  - A Fisher score iteration of 15-.., to much iterations to be reliable.

**NOTE:** if you think you detected overdispersion, check by:

```{r}
#Residual deviance: 238.88  on 70  degrees of freedom
pchisq(238.88,70, lower.tail = FALSE)
```

P-value < 0.05, there is a significant overdispersion.


***

















